{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and import dependencies\n",
    "\n",
    "## Installation\n",
    "\n",
    "The first step is to install all the necessary libraries. \n",
    "\n",
    "1. Machine Learning libraries:\n",
    "\n",
    "**- PyTorch:** For this project, I will use PyTorch, and as I am using an Apple silicon Mac, I will be running it on the MPS backend. \n",
    "\n",
    "**- Scikit-Learn:** This library is leveraged to split training and testing data and to quantify the precision or the likelihood of the speaker expressing a word on the real-time video feed.\n",
    "\n",
    "2. Computer Vision libraries:\n",
    "\n",
    "**- OpenCV** is an open-source Computer Vision library, in this project it is used to enable webcam activation.\n",
    "**- MediaPipe Holistic** is used to track hand and face features by extracting keypoints.\n",
    "\n",
    "3. Plotting library:\n",
    "\n",
    "**- Matplotlib** is used for visual representation of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install pytorch opencv-python mediapipe scikit-learn matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, clear_output\n",
    "# Voice\n",
    "from gtts import gTTS\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MediaPipe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holistic model to make detection\n",
    "mp_holistic = mp.solutions.holistic \n",
    "\n",
    "# Drawing utilities to draw detection\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "\n",
    "# Function to make the detection\n",
    "def mediapipe_detection(image, model):\n",
    "    # Convert colors from BGR to RGB (needs to be RGB to make detection in MediaPipe)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    # To save space --> make unwriteable image\n",
    "    image.flags.writeable = False  \n",
    "    # Make prediction\n",
    "    results = model.process(image) \n",
    "    # Convert back to writeable image\n",
    "    image.flags.writeable = True  \n",
    "    # Convert colors from BGR to RGB                 \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(236,138,26), thickness=1, circle_radius=1),  # dot color (remember colors are in BGR )  \n",
    "                             mp_drawing.DrawingSpec(color=(236,208,26), thickness=1, circle_radius=1)   # line color\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(186,52,25), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(236,103,26), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(0,153,0), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(51,255,51), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,0), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(255,255,102), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "def keypoints_extraction(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    # face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)                # multiplied by 3 as it has x, y and z components\n",
    "    left_hand = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)  \n",
    "    right_hand = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    return np.concatenate([pose, left_hand, right_hand]) # no face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Folder setup for data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, these are numpy arrays\n",
    "DATA_PATH = os.path.join('dataset') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['Hello','Thank you', 'I love you', 'How are you?', 'Nice to meet you!', \n",
    "                    \"What's your name?\", 'Deaf', 'Hearing', 'Hard of hearing', 'Goodbye', 'See you later','Sorry', \n",
    "                    'Please','Good', 'Fine', 'Bad','Excuse me', 'Good morning', 'Good night', \n",
    "                    'Hungry', 'Tired', 'Help', 'Busy', 'A','B','C','D','E','F','G','H','I','J',\n",
    "                    'K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','1','2','3',\n",
    "                    '4','5','7','8','10','11','12','13','14','15','20'])\n",
    "\n",
    "# Number of videos collected per action\n",
    "no_sequences = 60\n",
    "\n",
    "# 30 frames each video\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30\n",
    "\n",
    "# Create folders where data is stored\n",
    "for action in actions:      # loop through actions\n",
    "    for sequence in range(no_sequences):     # loop through videos (no_sequences,no_sequences+no_sequences): \n",
    "        try:    # just in case the directories are created already to avoid errors\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))  # a folder is created called 'MP_Data', then inside it another folder for each action, and within in action there will be a folder for each sequence/video\n",
    "        except: # if the folder is created, skip making the directory\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data collection: Training and Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences/videos\n",
    "        for sequence in range(no_sequences):  \n",
    "        \n",
    "            # Loop through video length\n",
    "            for frame_num in range(sequence_length):\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                # Make detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "                # Draw landmarks\n",
    "                draw_landmarks(image, results)\n",
    "                # Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), # x and y values of pixels where it is going to be displayed\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA) # font, font size, color in BGR, line width and line type\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,30), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('Webcam Feed', image)\n",
    "                    cv2.waitKey(1000)    # wait a second\n",
    "                else: # if not in frame 0\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,30), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('Webcam Feed', image)\n",
    "                \n",
    "                # Extract and export keypoints\n",
    "                keypoints = keypoints_extraction(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hello': 0, 'Thank you': 1, 'I love you': 2, 'How are you?': 3, 'Nice to meet you!': 4, \"What's your name?\": 5, 'Deaf': 6, 'Hearing': 7, 'Hard of hearing': 8, 'Goodbye': 9, 'See you later': 10, 'Sorry': 11, 'Please': 12, 'Good': 13, 'Fine': 14, 'Bad': 15, 'Excuse me': 16, 'Good morning': 17, 'Good night': 18, 'Hungry': 19, 'Tired': 20, 'Help': 21, 'Busy': 22, 'A': 23, 'B': 24, 'C': 25, 'D': 26, 'E': 27, 'F': 28, 'G': 29, 'H': 30, 'I': 31, 'J': 32, 'K': 33, 'L': 34, 'M': 35, 'N': 36, 'O': 37, 'P': 38, 'Q': 39, 'R': 40, 'S': 41, 'T': 42, 'U': 43, 'V': 44, 'W': 45, 'X': 46, 'Y': 47, 'Z': 48, '1': 49, '2': 50, '3': 51, '4': 52, '5': 53, '7': 54, '8': 55, '10': 56, '11': 57, '12': 58, '13': 59, '14': 60, '15': 61, '20': 62}\n"
     ]
    }
   ],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "print(label_map)\n",
    "\n",
    "# Load data\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    sequence_list = [item for item in os.listdir(os.path.join(DATA_PATH, action)) if item.isdigit()]\n",
    "    for sequence in np.array(sequence_list).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(sequences)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Create a dataset and split it\n",
    "dataset = TensorDataset(X, y)\n",
    "train_size = int(0.95 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3780, 30, 258)\n",
      "(3780,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(sequences).shape)\n",
    "print(np.array(labels).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Neural Network\n",
    "\n",
    "## Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device detected. Running on MPS.\n"
     ]
    }
   ],
   "source": [
    "class SignLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SignLanguageModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(258, 128, batch_first=True, bidirectional=True)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.gru1 = nn.GRU(256, 128, batch_first=True, bidirectional=True)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, actions.shape[0])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x.transpose(1, 2)  # Transpose to (batch_size, features, sequence_length)\n",
    "        x = self.bn1(x)\n",
    "        x = x.transpose(1, 2)  # Transpose back to (batch_size, sequence_length, features)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x, _ = self.gru1(x)\n",
    "        x = x.transpose(1, 2)  # Transpose to (batch_size, features, sequence_length)\n",
    "        x = self.bn2(x)\n",
    "        x = x.transpose(1, 2)  # Transpose back to (batch_size, sequence_length, features)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = x[:, -1, :]  # Take the last time step\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x  # Raw logits\n",
    "\n",
    "# Set device to CUDA or MPS\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA device detected. Running on CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"MPS device detected. Running on MPS.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SignLanguageModel                        [1, 63]                   --\n",
       "├─LSTM: 1-1                              [1, 30, 256]              397,312\n",
       "├─BatchNorm1d: 1-2                       [1, 256, 30]              512\n",
       "├─Dropout: 1-3                           [1, 30, 256]              --\n",
       "├─GRU: 1-4                               [1, 30, 256]              296,448\n",
       "├─BatchNorm1d: 1-5                       [1, 256, 30]              512\n",
       "├─Dropout: 1-6                           [1, 30, 256]              --\n",
       "├─Linear: 1-7                            [1, 128]                  32,896\n",
       "├─Dropout: 1-8                           [1, 128]                  --\n",
       "├─Linear: 1-9                            [1, 64]                   8,256\n",
       "├─Linear: 1-10                           [1, 63]                   4,095\n",
       "==========================================================================================\n",
       "Total params: 740,031\n",
       "Trainable params: 740,031\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 20.86\n",
       "==========================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 0.25\n",
       "Params size (MB): 2.96\n",
       "Estimated Total Size (MB): 3.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print summary of the model\n",
    "model = SignLanguageModel().to(device)\n",
    "print(\"Model Summary:\")\n",
    "summary(model, input_size=(1, 30, 258)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model, loss function, optimizer, and scheduler\n",
    "model = SignLanguageModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Reduced learning rate\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Optimize data loading\n",
    "num_workers = 10  # Adjust based on your CPU cores\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# Enable interactive mode in matplotlib\n",
    "plt.ion()\n",
    "\n",
    "# Create figures and axes for live plotting\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "num_epochs = 47\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n",
    "\n",
    "start_time_total = time.time()  # Start the overall timer\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()  # Start the epoch timer\n",
    "    model.train()\n",
    "    running_loss, correct = 0.0, 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_losses.append(running_loss / len(train_loader.dataset))\n",
    "    train_accuracies.append(100 * correct / len(train_loader.dataset))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            test_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    test_losses.append(test_loss / len(test_loader.dataset))\n",
    "    test_accuracies.append(100 * correct / len(test_loader.dataset))\n",
    "    \n",
    "    scheduler.step(test_losses[-1])  # Step the scheduler with the test loss\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time  # Time taken for the epoch\n",
    "    overall_time = time.time() - start_time_total  # Overall time taken\n",
    "    \n",
    "    # Print epoch results with timing information\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}, Test Acc: {test_accuracies[-1]:.2f}, Epoch Time: {epoch_time:.2f} sec, Overall Time: {overall_time:.2f} sec')\n",
    "\n",
    "    # Clear previous plot and update\n",
    "    ax1.clear()\n",
    "    ax1.plot(range(1, epoch+2), train_losses, label='Train Loss')\n",
    "    ax1.plot(range(1, epoch+2), test_losses, label='Test Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.set_title('Loss vs Epoch')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.plot(range(1, epoch+2), train_accuracies, label='Train Accuracy')\n",
    "    ax2.plot(range(1, epoch+2), test_accuracies, label='Test Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.set_title('Accuracy vs Epoch')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.pause(0.1)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "# Disable interactive mode and show final plot\n",
    "plt.ioff()\n",
    "\n",
    "print(f'Last epoch {num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}, Test Acc: {test_accuracies[-1]:.2f}, Overall Time: {overall_time:.2f} sec')\n",
    "\n",
    "fig1, (ax3, ax4) = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "ax3.plot(range(1, epoch+2), train_losses, label='Train Loss')\n",
    "ax3.plot(range(1, epoch+2), test_losses, label='Test Loss')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.legend()\n",
    "ax3.set_title('Loss vs Epoch')\n",
    "\n",
    "ax4.plot(range(1, epoch+2), train_accuracies, label='Train Accuracy')\n",
    "ax4.plot(range(1, epoch+2), test_accuracies, label='Test Accuracy')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.legend()\n",
    "ax4.set_title('Accuracy vs Epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "display(fig1)\n",
    "plt.show()\n",
    "\n",
    "# Improved prediction example\n",
    "model.eval()\n",
    "total_time = 0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):  # Show 5 examples\n",
    "        X_batch, y_batch = next(iter(test_loader))\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        outputs = model(X_batch)\n",
    "        end_time = time.time()\n",
    "\n",
    "        batch_time = end_time - start_time\n",
    "        total_time += batch_time\n",
    "        num_batches += 1\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        print(f'Predicted: {actions[predicted[i]]}, Actual: {actions[y_batch[i]]}')\n",
    "\n",
    "average_time_per_batch = total_time / num_batches\n",
    "fps = 1 / average_time_per_batch\n",
    "\n",
    "print(f'Average FPS: {fps:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: G, Actual: G\n",
      "Predicted: T, Actual: T\n",
      "Predicted: Nice to meet you!, Actual: Nice to meet you!\n",
      "Predicted: Good night, Actual: Good night\n",
      "Predicted: I love you, Actual: I love you\n",
      "Average Inference Time per Batch: 0.068118 seconds\n",
      "Average FPS: 14.68\n"
     ]
    }
   ],
   "source": [
    "# Improved prediction example with inference speed estimation\n",
    "\n",
    "# Load the model\n",
    "model = SignLanguageModel()\n",
    "model.load_state_dict(torch.load('sign_language_model_weights_v4_80e_ds4_v3.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "total_time = 0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(5):  # Show 5 examples\n",
    "        X_batch, y_batch = next(iter(test_loader))\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        outputs = model(X_batch)\n",
    "        end_time = time.time()\n",
    "\n",
    "        batch_time = end_time - start_time\n",
    "        total_time += batch_time\n",
    "        num_batches += 1\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        print(f'Predicted: {actions[predicted[i]]}, Actual: {actions[y_batch[i]]}')\n",
    "\n",
    "# Calculate average time per batch and FPS (frames per second)\n",
    "average_time_per_batch = total_time / num_batches\n",
    "fps = 1 / average_time_per_batch\n",
    "\n",
    "print(f'Average Inference Time per Batch: {average_time_per_batch:.6f} seconds')\n",
    "print(f'Average FPS: {fps:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'sign_language_model_weights_v4_80e_ds4_v3.pth')\n",
    "print(\"Model weights saved successfully.\")\n",
    "\n",
    "# To load the weights\n",
    "#model = SignLanguageModel()\n",
    "#model.load_state_dict(torch.load('sign_language_model_weights.pth'))\n",
    "#model.to(device)\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Text-to-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice --> Pre generate so there is not lag!\n",
    "\n",
    "# Directory for pre-generated audios \n",
    "os.makedirs(\"pre_gen_audios\", exist_ok=True)\n",
    "\n",
    "# Pre-generate audio files\n",
    "for word in actions:\n",
    "    tts = gTTS(text=word, lang='en')\n",
    "    tts.save(f\"pre_gen_audios/{word}.mp3\")\n",
    "\n",
    "# Function to play audio in a separate thread so it can run offline and no API lag\n",
    "def play_audio_async(audio_file):\n",
    "    os.system(f\"afplay {audio_file}\")\n",
    "\n",
    "# Modified speak_text function\n",
    "def speak_text(text):\n",
    "    audio_file = f\"pre_gen_audios/{text}.mp3\"\n",
    "    if os.path.exists(audio_file):\n",
    "        thread = threading.Thread(target=play_audio_async, args=(audio_file,))\n",
    "        thread.start()\n",
    "    else:\n",
    "        print(f\"Audio file for '{text}' not found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Live Test\n",
    "\n",
    "### Improtant added the following to improve predictions\n",
    "\n",
    "\t1.Increased Threshold:\n",
    "    \t•\tThe threshold for displaying the predicted action has been increased to 0.9 for higher confidence.\n",
    "\t2. Adjusted MediaPipe Confidence Levels:\n",
    "    \t•\tmin_detection_confidence and min_tracking_confidence have been increased to 0.85 to ensure more reliable keypoint detections.\n",
    "\t3. Smoothing Predictions:\n",
    "    \t•\tA simple smoothing technique is implemented by using the most common prediction over the last 15 frames.\n",
    "    \t•\tOnly if the most common prediction appears more than 10 times in the last 15 frames and its confidence is above the threshold, it will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n",
      "may only specify one file to play\n",
      "\n",
      "    Audio File Play\n",
      "    Version: 2.0\n",
      "    Copyright 2003-2013, Apple Inc. All Rights Reserved.\n",
      "    Specify -h (-help) for command options\n",
      "\n",
      "Usage:\n",
      "afplay [option...] audio_file\n",
      "\n",
      "Options: (may appear before or after arguments)\n",
      "  {-v | --volume} VOLUME\n",
      "    set the volume for playback of the file\n",
      "  {-h | --help}\n",
      "    print help\n",
      "  { --leaks}\n",
      "    run leaks analysis\n",
      "  {-t | --time} TIME\n",
      "    play for TIME seconds\n",
      "  {-r | --rate} RATE\n",
      "    play at playback rate\n",
      "  {-q | --rQuality} QUALITY\n",
      "    set the quality used for rate-scaled playback (default is 0 - low quality, 1 - high quality)\n",
      "  {-d | --debug}\n",
      "    debug print output\n"
     ]
    }
   ],
   "source": [
    "#import cv2\n",
    "#import numpy as np\n",
    "#import torch\n",
    "#\n",
    "## Set device to CUDA or MPS\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device('cuda')\n",
    "#    print(\"CUDA device detected. Running on CUDA.\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device('mps')\n",
    "#    print(\"MPS device detected. Running on MPS.\")\n",
    "#else:\n",
    "#    device = torch.device('cpu')\n",
    "#    print(\"No GPU detected. Running on CPU.\")\n",
    "#\n",
    "## Define the actions array as in your training\n",
    "#actions = np.array(['Hello','Thank you', 'I love you', 'How are you?', 'Nice to meet you!', \n",
    "#                    \"What's your name?\", 'Deaf', 'Hard of hearing', 'Goodbye','Sorry', 'Please',\n",
    "#                    'Good', 'Fine', 'Bad','Excuse me', 'Good morning', 'Good night', 'Hungry', \n",
    "#                    'Tired', 'Help', 'Busy', 'A','B','C','D','E','F','G','H','I','J','K','L','M',\n",
    "#                    'N','O','P','Q','R','S','T','U','V','W','X','Y','Z','1','2','3','4','5','10',\n",
    "#                    '11','12','13','14','15','20'])\n",
    "#\n",
    "\n",
    "def put_text_centered(image, text, font=cv2.FONT_HERSHEY_DUPLEX, font_scale=1, color=(0, 0, 255), thickness=4):\n",
    "    # Get the image dimensions\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # Calculate text size\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "\n",
    "    # Calculate the center position\n",
    "    x = (w - text_width) // 2\n",
    "    y = h - text_height - 10  # 10 pixels from the bottom\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(image, text, (x, y), font, font_scale, color, thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "# Load the model\n",
    "model = SignLanguageModel()\n",
    "model.load_state_dict(torch.load('sign_language_model_weights_v4_80e_ds4_v3.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "sequence = []\n",
    "threshold = 0.80  # Increased threshold for higher confidence\n",
    "smoothing_frames = 20\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "mp_holistic = mp.solutions.holistic \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # Prediction logic\n",
    "        keypoints = keypoints_extraction(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            input_data = np.expand_dims(sequence, axis=0)\n",
    "            input_data = torch.tensor(input_data, dtype=torch.float32).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                res = model(input_data)\n",
    "                res = res.cpu().numpy()[0]\n",
    "                predictions.append(np.argmax(res))\n",
    "                \n",
    "                # Implementing simple smoothing\n",
    "                if len(predictions) >= smoothing_frames:\n",
    "                    most_common_pred = np.bincount(predictions[-smoothing_frames:]).argmax()\n",
    "                    if np.bincount(predictions[-smoothing_frames:])[most_common_pred] > 12 and res[most_common_pred] > threshold:\n",
    "                        sentence = str(actions[most_common_pred])\n",
    "                        put_text_centered(image, sentence, font_scale=2, color=(0, 0, 255), thickness=4)\n",
    "                        speak_text(str(actions[most_common_pred]))\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('Webcam Feed', image)\n",
    "        \n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env_sign_language",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
